---
phase: 07-migration-consolidation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/alembic/versions/001_initial_schema.py
  - backend/alembic/versions/001_initial_schema.sql
autonomous: true

must_haves:
  truths:
    - "Consolidated migration captures entire database schema"
    - "Migration includes all tables, indexes, triggers, functions, and materialized views"
    - "Migration includes controlled vocabulary data (sex_values, etc.)"
  artifacts:
    - path: "backend/alembic/versions/001_initial_schema.py"
      provides: "Consolidated Alembic migration"
      min_lines: 50
      contains: "def upgrade"
    - path: "backend/alembic/versions/001_initial_schema.sql"
      provides: "Schema SQL extracted from pg_dump"
      min_lines: 500
  key_links:
    - from: "backend/alembic/versions/001_initial_schema.py"
      to: "backend/alembic/versions/001_initial_schema.sql"
      via: "Path(__file__).parent / '001_initial_schema.sql'"
      pattern: "001_initial_schema\\.sql"
---

<objective>
Extract current database schema and create consolidated migration file.

Purpose: Create a single migration that produces the exact same schema as running all 30 existing migrations. This will replace the migration chain for cleaner history and faster fresh database setup.

Output: New `001_initial_schema.py` migration file with accompanying SQL file containing the complete schema.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-migration-consolidation/07-CONTEXT.md
@.planning/phases/07-migration-consolidation/07-RESEARCH.md
@backend/alembic/versions/001_initial_phenopackets_v2_schema.py
@backend/alembic/versions/88b3a0c19a89_add_phenopacket_controlled_vocabularies.py
@backend/alembic/versions/b1e70338f190_populate_curated_hpo_terms_for_hnf1b.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extract schema SQL from current database</name>
  <files>backend/alembic/versions/001_initial_schema.sql</files>
  <action>
Extract the complete database schema using pg_dump:

1. Ensure database is running with current schema: `make hybrid-up` (if needed)
2. Run pg_dump to extract schema:
   ```bash
   cd backend
   pg_dump -s -O -x -U hnf1b_user -h localhost -p 5433 hnf1b_phenopackets > alembic/versions/001_initial_schema.sql
   ```

   Flags:
   - `-s`: schema-only (no data)
   - `-O`: no owner statements
   - `-x`: no privilege statements

3. Review the SQL file to ensure it contains:
   - All tables (phenopackets, users, families, cohorts, etc.)
   - Generated columns (subject_id, subject_sex, features_count)
   - All indexes (GIN, B-tree, composite)
   - Triggers (phenopackets_search_vector_trigger)
   - Functions (phenopackets_search_vector_update, refresh_all_aggregation_views)
   - Materialized views (mv_feature_aggregation, mv_disease_aggregation, etc.)
   - Extensions (pg_trgm)

4. Check if controlled vocabulary tables have data in the migration:
   - Look at migrations 88b3a0c19a89 and b1e70338f190
   - These INSERT data into sex_values, interpretation_status_values, etc.
   - This data MUST be included in the consolidated migration (add after schema)

5. Append INSERT statements for controlled vocabularies to the SQL file:
   - Copy INSERT statements from the data migrations
   - Place them AFTER the schema DDL statements
  </action>
  <verify>
```bash
# Check SQL file exists and has substantial content
wc -l backend/alembic/versions/001_initial_schema.sql
# Should be 500+ lines

# Check it contains key elements
grep -c "CREATE TABLE" backend/alembic/versions/001_initial_schema.sql
# Should show 10+ tables

grep -c "CREATE INDEX" backend/alembic/versions/001_initial_schema.sql
# Should show 10+ indexes

grep "pg_trgm" backend/alembic/versions/001_initial_schema.sql
# Should find pg_trgm extension
```
  </verify>
  <done>SQL file contains complete schema with 500+ lines, 10+ tables, 10+ indexes, and pg_trgm extension</done>
</task>

<task type="auto">
  <name>Task 2: Create consolidated migration Python file</name>
  <files>backend/alembic/versions/001_initial_schema.py</files>
  <action>
Create the Python migration file that executes the schema SQL:

1. Use this template:

```python
"""Consolidated initial schema

This migration consolidates all previous migrations into a single file.
Previous migrations are preserved in git history for reference.

Git reference for archived migrations:
  See: git log --oneline -- backend/alembic/versions/

Tables:
  - phenopackets (main data with JSONB storage)
  - users (authentication)
  - families, cohorts, resources
  - phenopacket_audit (change tracking)
  - hpo_terms_lookup (HPO term metadata)
  - publication_metadata (PubMed cache)
  - variant_annotations (VEP cache)
  - reference_genomes, genes, transcripts, exons, protein_domains
  - Controlled vocabulary tables: sex_values, interpretation_status_values,
    progress_status_values, allelic_state_values, evidence_code_values

Materialized Views:
  - mv_feature_aggregation
  - mv_disease_aggregation
  - mv_sex_distribution
  - mv_summary_statistics
  - global_search_index

Functions & Triggers:
  - phenopackets_search_vector_update()
  - refresh_all_aggregation_views()
  - phenopackets_search_vector_trigger

Extensions:
  - pg_trgm (trigram similarity)

Revision ID: 001_initial_schema
Revises:
Create Date: 2026-01-20
"""
from pathlib import Path

from alembic import op


# revision identifiers, used by Alembic.
revision = "001_initial_schema"
down_revision = None
branch_labels = None
depends_on = None


def upgrade() -> None:
    """Create the complete database schema."""
    # Load and execute consolidated schema SQL
    sql_file = Path(__file__).parent / "001_initial_schema.sql"
    sql = sql_file.read_text()

    # Execute statements individually (asyncpg requires single statements)
    # Split on semicolon followed by newline to handle multi-line statements
    for stmt in sql.split(";\n"):
        stmt = stmt.strip()
        if stmt and not stmt.startswith("--"):
            op.execute(stmt + ";")


def downgrade() -> None:
    """Drop all schema objects.

    WARNING: This will delete ALL data. Use with caution.
    """
    # Drop materialized views
    op.execute("DROP MATERIALIZED VIEW IF EXISTS global_search_index CASCADE;")
    op.execute("DROP MATERIALIZED VIEW IF EXISTS mv_summary_statistics CASCADE;")
    op.execute("DROP MATERIALIZED VIEW IF EXISTS mv_sex_distribution CASCADE;")
    op.execute("DROP MATERIALIZED VIEW IF EXISTS mv_disease_aggregation CASCADE;")
    op.execute("DROP MATERIALIZED VIEW IF EXISTS mv_feature_aggregation CASCADE;")

    # Drop functions (triggers depend on them)
    op.execute("DROP FUNCTION IF EXISTS refresh_all_aggregation_views() CASCADE;")
    op.execute("DROP FUNCTION IF EXISTS phenopackets_search_vector_update() CASCADE;")

    # Drop tables in dependency order
    op.execute("DROP TABLE IF EXISTS phenopacket_audit CASCADE;")
    op.execute("DROP TABLE IF EXISTS publication_metadata CASCADE;")
    op.execute("DROP TABLE IF EXISTS variant_annotations CASCADE;")
    op.execute("DROP TABLE IF EXISTS protein_domains CASCADE;")
    op.execute("DROP TABLE IF EXISTS exons CASCADE;")
    op.execute("DROP TABLE IF EXISTS transcripts CASCADE;")
    op.execute("DROP TABLE IF EXISTS genes CASCADE;")
    op.execute("DROP TABLE IF EXISTS reference_genomes CASCADE;")
    op.execute("DROP TABLE IF EXISTS hpo_terms_lookup CASCADE;")
    op.execute("DROP TABLE IF EXISTS resources CASCADE;")
    op.execute("DROP TABLE IF EXISTS families CASCADE;")
    op.execute("DROP TABLE IF EXISTS cohorts CASCADE;")
    op.execute("DROP TABLE IF EXISTS phenopackets CASCADE;")
    op.execute("DROP TABLE IF EXISTS users CASCADE;")

    # Drop controlled vocabulary tables
    op.execute("DROP TABLE IF EXISTS evidence_code_values CASCADE;")
    op.execute("DROP TABLE IF EXISTS allelic_state_values CASCADE;")
    op.execute("DROP TABLE IF EXISTS progress_status_values CASCADE;")
    op.execute("DROP TABLE IF EXISTS interpretation_status_values CASCADE;")
    op.execute("DROP TABLE IF EXISTS sex_values CASCADE;")

    # Drop extension
    op.execute("DROP EXTENSION IF EXISTS pg_trgm;")
```

2. Ensure the SQL file is read from the same directory as the migration file
3. Use semicolon + newline splitting to handle multi-line function definitions
  </action>
  <verify>
```bash
# Check migration file syntax
cd backend && uv run python -c "import alembic.versions; print('Syntax OK')" 2>/dev/null || \
cd backend && uv run python -c "
import ast
with open('alembic/versions/001_initial_schema.py') as f:
    ast.parse(f.read())
print('Syntax OK')
"
```
  </verify>
  <done>Migration file exists with valid Python syntax and correct revision ID</done>
</task>

<task type="auto">
  <name>Task 3: Verify SQL file completeness</name>
  <files></files>
  <action>
Perform a thorough review of the extracted SQL to ensure nothing is missing:

1. Check for all expected tables by comparing against existing migrations:
   - phenopackets (001_initial)
   - users (131bded2e26e)
   - hpo_terms_lookup (0bd1567a483c)
   - publication_metadata (8d988c04336a)
   - variant_annotations (7b2a3c4d5e6f)
   - reference tables: genes, transcripts, exons, protein_domains (62362ff6f580)
   - controlled vocabulary tables (88b3a0c19a89)

2. Check for all indexes:
   - JSONB GIN indexes (002, 003)
   - Full-text search indexes (8baf0de6a441, f74b2759f2a9)
   - Global search indexes (5f9c34e4e444, 6a1b2c3d4e5f, a1b2c3d4e5f6)
   - Cursor pagination index (72e990f17d42)

3. Check for triggers and functions:
   - phenopackets_search_vector_update function
   - phenopackets_search_vector_trigger

4. Check for materialized views:
   - mv_feature_aggregation
   - mv_disease_aggregation
   - mv_sex_distribution
   - mv_summary_statistics
   - global_search_index

5. If anything is missing, manually add it to the SQL file from the relevant migration

6. Verify controlled vocabulary INSERT statements are present at the end
  </action>
  <verify>
```bash
# Quick completeness check
cd backend
echo "=== Tables ==="
grep -c "CREATE TABLE" alembic/versions/001_initial_schema.sql

echo "=== Materialized Views ==="
grep -c "CREATE MATERIALIZED VIEW" alembic/versions/001_initial_schema.sql

echo "=== Functions ==="
grep -c "CREATE.*FUNCTION" alembic/versions/001_initial_schema.sql

echo "=== Triggers ==="
grep -c "CREATE TRIGGER" alembic/versions/001_initial_schema.sql

echo "=== Controlled Vocabulary INSERTs ==="
grep -c "INSERT INTO" alembic/versions/001_initial_schema.sql
```
  </verify>
  <done>SQL file contains all tables, 5 materialized views, 2+ functions, 1+ trigger, and INSERT statements for vocabularies</done>
</task>

</tasks>

<verification>
1. `backend/alembic/versions/001_initial_schema.sql` exists with 500+ lines
2. `backend/alembic/versions/001_initial_schema.py` has valid Python syntax
3. SQL contains CREATE TABLE statements for all expected tables
4. SQL contains CREATE MATERIALIZED VIEW for all 5 MVs
5. SQL contains function and trigger definitions
6. SQL contains INSERT statements for controlled vocabularies
7. Migration revision ID is "001_initial_schema" with down_revision = None
</verification>

<success_criteria>
- [ ] 001_initial_schema.sql exists with complete schema (500+ lines)
- [ ] 001_initial_schema.py exists with correct structure
- [ ] All tables present (phenopackets, users, families, etc.)
- [ ] All materialized views present (5 total)
- [ ] Functions and triggers present
- [ ] Controlled vocabulary data included
- [ ] Python migration file has valid syntax
</success_criteria>

<output>
After completion, create `.planning/phases/07-migration-consolidation/07-02-SUMMARY.md`
</output>
